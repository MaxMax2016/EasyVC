# experiment
dataset: libritts
train_meta: data/libritts/train_nodev_clean/metadata.csv
dev_meta: data/libritts/dev_clean/metadata.csv
train_set: train_nodev_clean
dev_set: dev_clean


# encoder-decoder
ling_enc: hubert_soft
spk_enc: utt_dvec
pros_enc: f0
decoder: TacoMOL


# training 
fp16_run: !!bool True
epochs: 200
save_freq: 2 # save ckpt frequency
show_freq: 10
load_only_params: !!bool False
seed: !!int 1234
trainer: TacoMOLTrainer
ngpu: 2

#dataloader
sort: !!bool True
dump_dir: dump
num_workers: !!int 8
batch_size: 32
drop_last: !!bool True
rm_long_utt: !!bool True # remove too long utterances from metadata
max_utt_duration: !!float 10.0 # max utterance duration (seconds)
frames_per_step: !!int 4


# decoder params
decoder_params: 
  spk_embed_dim: 256
  bottle_neck_feature_dim: 256

#optimizer & scheduler
optimizer:
    weight_decay: !!float 1e-6
    betas: [0.9,0.99]  
    lr: !!float 1e-4
scheduler:    
    num_training_steps: 500000
    num_warmup_steps: 4000

# loss hyper-parameters
loss:
   alpha: 1. 
    






